filename: 003_albumentations_smoothing.ipynb, model: efficientnet-b5, lr: 1e-05, weights: tensor([1., 1., 1., 1., 1.]). batchsize: 2, kfold: 3, epoch: 25, weght_decay: 5e-05, smoothing: 0.1

kfold: 1, epoch: 1. train_loss: 1.2129862870534216, train_acc: 62.52103196859226. val_loss: 1.3294164122136862, val_acc: 60.549558390579
kfold: 1, epoch: 2. train_loss: 1.0916745765066789, train_acc: 67.84913067863152. val_loss: 1.12687722367334, val_acc: 69.67615309126595
kfold: 1, epoch: 3. train_loss: 1.030562486029447, train_acc: 72.16068424004487. val_loss: 1.0092206335816358, val_acc: 73.83989906070377
kfold: 1, epoch: 4. train_loss: 0.9460226219195499, train_acc: 76.05159842961301. val_loss: 0.978714527679753, val_acc: 77.70923874947428
kfold: 1, epoch: 5. train_loss: 0.8711600253923649, train_acc: 79.57795849691531. val_loss: 0.9110094789182702, val_acc: 79.78410206084396
kfold: 1, epoch: 6. train_loss: 0.8406708689048192, train_acc: 80.99411104879417. val_loss: 0.8446028979261749, val_acc: 83.94784803028179
kfold: 1, epoch: 7. train_loss: 0.8133570992764434, train_acc: 82.31912507010657. val_loss: 0.8448055047413399, val_acc: 84.60675732510865
kfold: 1, epoch: 8. train_loss: 0.7964163676063064, train_acc: 83.63011777902412. val_loss: 0.8261255248942427, val_acc: 85.13949249964952
kfold: 1, epoch: 9. train_loss: 0.7801705169890011, train_acc: 84.12787436904094. val_loss: 0.8112794153657586, val_acc: 84.17215757745689
kfold: 1, epoch: 10. train_loss: 0.7730549933974888, train_acc: 84.7728547392036. val_loss: 0.8103336740315445, val_acc: 84.48058320482265
kfold: 1, epoch: 11. train_loss: 0.7774356272462306, train_acc: 84.14189568143578. val_loss: 0.8655160832862945, val_acc: 85.0413570727604
kfold: 1, epoch: 12. train_loss: 0.7678318565363504, train_acc: 84.7097588334268. val_loss: 0.8445686597282024, val_acc: 84.64881536520399
kfold: 1, epoch: 13. train_loss: 0.7606295684934398, train_acc: 84.9831744251262. val_loss: 0.844167600944325, val_acc: 85.08341511285575
kfold: 1, epoch: 14. train_loss: 0.7624875454757565, train_acc: 84.96915311273135. val_loss: 0.8259448129354754, val_acc: 84.60675732510865
kfold: 1, epoch: 15. train_loss: 0.7608724347557569, train_acc: 85.06029164329782. val_loss: 0.8368420984803845, val_acc: 86.19094350203281
kfold: 1, epoch: 16. train_loss: 0.76500955818245, train_acc: 84.6396522714526. val_loss: 0.9159151032287132, val_acc: 84.13009953736156
kfold: 1, epoch: 17. train_loss: 0.7714288014160136, train_acc: 84.65367358384745. val_loss: 0.9983389079904971, val_acc: 84.35440908453666
kfold: 1, epoch: 18. train_loss: 0.770168778948816, train_acc: 84.62563095905777. val_loss: 1.2070287811445328, val_acc: 83.76559652320202
kfold: 1, epoch: 19. train_loss: 0.7778505762925124, train_acc: 84.00869321368481. val_loss: 0.9603576869031512, val_acc: 85.32174400672929
kfold: 1, epoch: 20. train_loss: 0.7850319864463419, train_acc: 83.58104318564217. val_loss: 1.1019126539441562, val_acc: 80.24674050189262
kfold: 1, epoch: 21. train_loss: 0.7948514405665141, train_acc: 83.12535053280988. val_loss: 1.0315778005814933, val_acc: 84.04598345717089
kfold: 1, epoch: 22. train_loss: 0.7953578800347436, train_acc: 83.06225462703308. val_loss: 1.3110983823824236, val_acc: 81.98513949249966
kfold: 1, epoch: 23. train_loss: 0.8107353104001608, train_acc: 82.1228266965788. val_loss: 1.770300646071545, val_acc: 80.13458572830505
kfold: 1, epoch: 24. train_loss: 0.8226508756099761, train_acc: 81.7372406057207. val_loss: 0.9070241936021136, val_acc: 82.93845506799383
kfold: 1, epoch: 25. train_loss: 0.8363441319919943, train_acc: 81.13432417274257. val_loss: 2.2725365529683312, val_acc: 78.52236085798401
kfold: 2, epoch: 1. train_loss: 1.2243333367483822, train_acc: 62.07500876270592. val_loss: 1.2610121975270003, val_acc: 62.77341559169938
kfold: 2, epoch: 2. train_loss: 1.0896862910068938, train_acc: 68.04065895548545. val_loss: 1.1666685166532111, val_acc: 63.9652271452608
kfold: 2, epoch: 3. train_loss: 1.017419976463128, train_acc: 71.95934104451455. val_loss: 1.0099008539487857, val_acc: 73.87829500841279
kfold: 2, epoch: 4. train_loss: 0.9401239778631852, train_acc: 75.84297230984929. val_loss: 0.9129284459573127, val_acc: 79.2624789680314
kfold: 2, epoch: 5. train_loss: 0.8813863693665074, train_acc: 78.81528215913075. val_loss: 0.9621306367540252, val_acc: 78.33707234997196
kfold: 2, epoch: 6. train_loss: 0.842039752128531, train_acc: 81.05853487556958. val_loss: 0.8781015412130854, val_acc: 82.83791362871565
kfold: 2, epoch: 7. train_loss: 0.8171945525831784, train_acc: 81.81563266736768. val_loss: 0.9189372963104365, val_acc: 82.13684800897364
kfold: 2, epoch: 8. train_loss: 0.8028307410496809, train_acc: 83.06344199088679. val_loss: 0.808661172430768, val_acc: 85.09534492428492
kfold: 2, epoch: 9. train_loss: 0.7866160992434261, train_acc: 83.90466175955135. val_loss: 0.899213734623494, val_acc: 83.45485137408862
kfold: 2, epoch: 10. train_loss: 0.7818391511187252, train_acc: 83.869610935857. val_loss: 0.8152589519757336, val_acc: 85.26360067302299
kfold: 2, epoch: 11. train_loss: 0.7767458085958309, train_acc: 84.26218016123379. val_loss: 0.8587239564585004, val_acc: 82.89399887829501
kfold: 2, epoch: 12. train_loss: 0.7700413287346082, train_acc: 84.4794952681388. val_loss: 0.7659478963123435, val_acc: 86.94615816040381
kfold: 2, epoch: 13. train_loss: 0.7673453885910974, train_acc: 84.45846477392219. val_loss: 0.8108085832081681, val_acc: 85.93662366797533