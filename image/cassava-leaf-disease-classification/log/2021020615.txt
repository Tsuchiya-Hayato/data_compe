filename: 003_albumentations_smoothing.ipynb, model: efficientnet-b4, lr: 1e-05, weights: tensor([1.5000, 1.0000, 1.0000, 1.0000, 1.0000]). batchsize: 4, kfold: 3, epoch: 25, weght_decay: 5e-05, smoothing: 0.1

kfold: 1, epoch: 1. train_loss: 1.2136028260911909, train_acc: 65.33931575995513. val_loss: 1.0117757728682506, val_acc: 77.02229076125053
kfold: 1, epoch: 2. train_loss: 1.0430175844231016, train_acc: 74.40409422321929. val_loss: 0.9074835090693337, val_acc: 81.20005607738679
kfold: 1, epoch: 3. train_loss: 0.9776798216934226, train_acc: 77.67106001121705. val_loss: 0.8562163851921334, val_acc: 83.35903546894714
kfold: 1, epoch: 4. train_loss: 0.944004683650349, train_acc: 79.24845765563657. val_loss: 0.8454221531481486, val_acc: 83.84971260339269
kfold: 1, epoch: 5. train_loss: 0.9249886105519229, train_acc: 80.03365114974761. val_loss: 0.8321509856079191, val_acc: 84.88714425907753
kfold: 1, epoch: 6. train_loss: 0.8971922473665441, train_acc: 81.28855860908581. val_loss: 0.820559658811766, val_acc: 85.26566661993552
kfold: 1, epoch: 7. train_loss: 0.8821537664647993, train_acc: 82.2279865395401. val_loss: 0.8091266930704694, val_acc: 85.60213094069816
kfold: 1, epoch: 8. train_loss: 0.8697223656811343, train_acc: 82.65563656758273. val_loss: 0.8203322327123629, val_acc: 85.67222767419038
kfold: 1, epoch: 9. train_loss: 0.8553141752823109, train_acc: 83.35670218732473. val_loss: 0.7950491094335312, val_acc: 86.26104023552503
kfold: 1, epoch: 10. train_loss: 0.8438535182414534, train_acc: 84.07178911946158. val_loss: 0.7997892036900393, val_acc: 85.99467264825459
kfold: 1, epoch: 11. train_loss: 0.8318478188451862, train_acc: 84.07178911946158. val_loss: 0.7895925412530856, val_acc: 86.40123370250946
kfold: 1, epoch: 12. train_loss: 0.8293792638662352, train_acc: 84.7378014582165. val_loss: 0.790783256091879, val_acc: 86.51338847609702
kfold: 1, epoch: 13. train_loss: 0.8255758543430388, train_acc: 84.92708917554683. val_loss: 0.7959859696271173, val_acc: 86.3171176223188
kfold: 1, epoch: 14. train_loss: 0.8166575211523893, train_acc: 85.26360067302299. val_loss: 0.791511625920176, val_acc: 86.40123370250946
kfold: 1, epoch: 15. train_loss: 0.8080176105675775, train_acc: 85.76135726303983. val_loss: 0.7841900153411343, val_acc: 86.84985279685966
kfold: 1, epoch: 16. train_loss: 0.8072448196667871, train_acc: 85.5440269209198. val_loss: 0.8000597206065473, val_acc: 86.68162063647834
kfold: 1, epoch: 17. train_loss: 0.7973770580964348, train_acc: 86.22406057206955. val_loss: 0.7949417357086601, val_acc: 86.87789149025656
kfold: 1, epoch: 18. train_loss: 0.7964816833739192, train_acc: 86.20302860347728. val_loss: 0.7997115233019328, val_acc: 86.55544651619235
kfold: 1, epoch: 19. train_loss: 0.7920385811914571, train_acc: 86.22406057206955. val_loss: 0.7965529063223723, val_acc: 87.03210430393943
kfold: 1, epoch: 20. train_loss: 0.7888364673362845, train_acc: 86.75687044307347. val_loss: 0.7969437733693507, val_acc: 87.31249123790832
kfold: 1, epoch: 21. train_loss: 0.7832155139407481, train_acc: 86.91110487941671. val_loss: 0.7876896717222283, val_acc: 87.14425907752698
kfold: 1, epoch: 22. train_loss: 0.7822072657219025, train_acc: 86.90409422321929. val_loss: 0.8146683014762242, val_acc: 86.65358194308145
kfold: 1, epoch: 23. train_loss: 0.7752451485819986, train_acc: 87.34576556365676. val_loss: 0.7965100994960075, val_acc: 87.48072339828964
kfold: 1, epoch: 24. train_loss: 0.7723677362442284, train_acc: 87.48597868760515. val_loss: 0.8025765051823026, val_acc: 86.94798822374878
kfold: 1, epoch: 25. train_loss: 0.7703690423566976, train_acc: 87.37380818844643. val_loss: 0.795594070023218, val_acc: 86.70965932987522
kfold: 2, epoch: 1. train_loss: 1.2073544860220369, train_acc: 66.49141254819489. val_loss: 1.0155589906472673, val_acc: 77.60796410544027
kfold: 2, epoch: 2. train_loss: 1.037071325480186, train_acc: 74.74237644584647. val_loss: 0.9002868745411238, val_acc: 81.82837913628715
kfold: 2, epoch: 3. train_loss: 0.9911040555939542, train_acc: 76.78934454959692. val_loss: 0.8578201398646237, val_acc: 83.23051037577117
kfold: 2, epoch: 4. train_loss: 0.9515173260090162, train_acc: 78.64703820539782. val_loss: 0.8273590371906925, val_acc: 84.96915311273135
kfold: 2, epoch: 5. train_loss: 0.9254645653422019, train_acc: 80.43463021381002. val_loss: 0.8194182934014903, val_acc: 85.30566461020751
kfold: 2, epoch: 6. train_loss: 0.9027525910249704, train_acc: 81.31791097090782. val_loss: 0.8074579434119378, val_acc: 85.5440269209198
kfold: 2, epoch: 7. train_loss: 0.8868153494447762, train_acc: 82.08902909218367. val_loss: 0.7963286601487533, val_acc: 86.27313516545149
kfold: 2, epoch: 8. train_loss: 0.8780316442109171, train_acc: 82.4745881528216. val_loss: 0.7974982206499182, val_acc: 86.58160403813797
kfold: 2, epoch: 9. train_loss: 0.8589513346465121, train_acc: 83.33683841570277. val_loss: 0.7832088568072872, val_acc: 87.26864834548513
kfold: 2, epoch: 10. train_loss: 0.8500145134185254, train_acc: 83.5471433578689. val_loss: 0.7893017850695068, val_acc: 86.74985978687606
kfold: 2, epoch: 11. train_loss: 0.8387560192396368, train_acc: 84.31826147914477. val_loss: 0.7707395484389981, val_acc: 87.57711721817162
kfold: 2, epoch: 12. train_loss: 0.8323611525016102, train_acc: 84.85804416403785. val_loss: 0.7731143482200764, val_acc: 87.77341559169939
kfold: 2, epoch: 13. train_loss: 0.8313113329355075, train_acc: 84.3813529617946. val_loss: 0.7760889554251063, val_acc: 87.50701065619742
kfold: 2, epoch: 14. train_loss: 0.8235435680344487, train_acc: 85.11041009463722. val_loss: 0.777386494854228, val_acc: 87.6752664049355
kfold: 2, epoch: 15. train_loss: 0.8134548611456451, train_acc: 85.45390816684193. val_loss: 0.7716622890479365, val_acc: 87.52103196859225
kfold: 2, epoch: 16. train_loss: 0.8124037887624308, train_acc: 85.51699964949177. val_loss: 0.7712692623384468, val_acc: 87.82950084127874
kfold: 2, epoch: 17. train_loss: 0.80374936645426, train_acc: 86.1198738170347. val_loss: 0.7811673218389609, val_acc: 87.66124509254067
kfold: 2, epoch: 18. train_loss: 0.8002353386949818, train_acc: 86.06379249912374. val_loss: 0.7730917092861918, val_acc: 87.61918115535615
kfold: 2, epoch: 19. train_loss: 0.8000818999467993, train_acc: 86.1689449702068. val_loss: 0.776340702946642, val_acc: 87.95569265283231
kfold: 2, epoch: 20. train_loss: 0.7902819448938415, train_acc: 86.56852436032247. val_loss: 0.7817943509014149, val_acc: 87.57711721817162
kfold: 2, epoch: 21. train_loss: 0.7907883174986875, train_acc: 86.47038205397827. val_loss: 0.7833864022625074, val_acc: 87.57711721817162
kfold: 2, epoch: 22. train_loss: 0.7842363892298037, train_acc: 86.54048370136698. val_loss: 0.7818131521385971, val_acc: 88.02579921480651
kfold: 2, epoch: 23. train_loss: 0.7854624537650297, train_acc: 86.89800210304942. val_loss: 0.7910711454061464, val_acc: 87.61918115535615
kfold: 2, epoch: 24. train_loss: 0.7803679695141426, train_acc: 86.96810375043813. val_loss: 0.7879037522427232, val_acc: 87.56309590577678
kfold: 2, epoch: 25. train_loss: 0.7765754303044554, train_acc: 87.28356116368735. val_loss: 0.7859767107099745, val_acc: 88.02579921480651
kfold: 3, epoch: 1. train_loss: 1.2170909116807558, train_acc: 65.0403084472485. val_loss: 1.028112229108142, val_acc: 76.02355580482333
kfold: 3, epoch: 2. train_loss: 1.0422730709378847, train_acc: 74.41289870311952. val_loss: 0.8898627778083771, val_acc: 81.80033651149748
kfold: 3, epoch: 3. train_loss: 0.9844781179738239, train_acc: 77.37819838766211. val_loss: 0.8421161358970239, val_acc: 83.95961862030286
kfold: 3, epoch: 4. train_loss: 0.951177029957917, train_acc: 78.84332281808622. val_loss: 0.8297524413431353, val_acc: 85.1233875490746
kfold: 3, epoch: 5. train_loss: 0.9178373611103696, train_acc: 80.93936207500876. val_loss: 0.830707634224092, val_acc: 85.34772854739204
kfold: 3, epoch: 6. train_loss: 0.8989890115515738, train_acc: 81.65439887837364. val_loss: 0.8256996580876464, val_acc: 85.33370723499719
kfold: 3, epoch: 7. train_loss: 0.8776668781220897, train_acc: 82.50963897651594. val_loss: 0.8085052238978901, val_acc: 86.2450925406618
kfold: 3, epoch: 8. train_loss: 0.8672616805788312, train_acc: 82.81808622502629. val_loss: 0.8135984756273964, val_acc: 85.5440269209198
kfold: 3, epoch: 9. train_loss: 0.8584366639120959, train_acc: 83.2316859446197. val_loss: 0.804937721903379, val_acc: 86.39932697700505
kfold: 3, epoch: 10. train_loss: 0.8475330915142248, train_acc: 83.81352961794602. val_loss: 0.795934964616982, val_acc: 86.58160403813797
kfold: 3, epoch: 11. train_loss: 0.8414000128877575, train_acc: 84.3322818086225. val_loss: 0.7999448204267847, val_acc: 86.81996634885026
kfold: 3, epoch: 12. train_loss: 0.8336809889121252, train_acc: 84.53557658604977. val_loss: 0.8019037769874428, val_acc: 86.23107122826697
kfold: 3, epoch: 13. train_loss: 0.8214157981835128, train_acc: 85.11041009463722. val_loss: 0.7922030195585477, val_acc: 86.97420078519349
kfold: 3, epoch: 14. train_loss: 0.8133384148079306, train_acc: 85.46792849631966. val_loss: 0.7882021896904664, val_acc: 87.5350532809871
kfold: 3, epoch: 15. train_loss: 0.8119032989705949, train_acc: 85.71328426218017. val_loss: 0.7838901880746201, val_acc: 87.64722378014582
kfold: 3, epoch: 16. train_loss: 0.8052980148026948, train_acc: 85.88152821591308. val_loss: 0.7987927066029555, val_acc: 87.40886146943353
kfold: 3, epoch: 17. train_loss: 0.7989989929851578, train_acc: 86.09884332281808. val_loss: 0.8041020904239884, val_acc: 86.90409422321929
kfold: 3, epoch: 18. train_loss: 0.7941378066611283, train_acc: 86.40028040658956. val_loss: 0.7946654797536365, val_acc: 87.56309590577678
kfold: 3, epoch: 19. train_loss: 0.7939931413089323, train_acc: 86.42832106554503. val_loss: 0.7872404791435631, val_acc: 87.78743690409422
kfold: 3, epoch: 20. train_loss: 0.7866249076117646, train_acc: 86.70171749036102. val_loss: 0.7870442625918685, val_acc: 87.54907459338195
kfold: 3, epoch: 21. train_loss: 0.7848111862958497, train_acc: 86.75078864353313. val_loss: 0.7953771005436335, val_acc: 87.50701065619742
kfold: 3, epoch: 22. train_loss: 0.7813959026791183, train_acc: 87.17840869260428. val_loss: 0.7923946512995713, val_acc: 87.71733034212002
kfold: 3, epoch: 23. train_loss: 0.7749667444381522, train_acc: 87.2204696810375. val_loss: 0.7988658408669361, val_acc: 87.47896803140775
kfold: 3, epoch: 24. train_loss: 0.7739774969936186, train_acc: 87.44479495268139. val_loss: 0.79441200496772, val_acc: 87.3948401570387
kfold: 3, epoch: 25. train_loss: 0.7651544793801112, train_acc: 87.77427269540834. val_loss: 0.7907514727924623, val_acc: 87.68928771733034