filename: 003_albumentations_smoothing.ipynb, model: efficientnet-b5, lr: 0.0001, weights: tensor([1.5000, 1.0000, 1.0000, 1.0000, 1.0000]). batchsize: 2, kfold: 3, epoch: 25, weght_decay: 5e-05, smoothing: 0.1

kfold: 1, epoch: 1. train_loss: 1.0742618855521442, train_acc: 72.68648345485137. val_loss: 1.0893777471515087, val_acc: 77.0783681480443
kfold: 1, epoch: 2. train_loss: 0.9050845737896538, train_acc: 80.58048233314638. val_loss: 1.074587657567263, val_acc: 79.53175382027197
kfold: 1, epoch: 3. train_loss: 0.870711154980868, train_acc: 82.55748738081884. val_loss: 0.9709079842424406, val_acc: 82.54591336043741
kfold: 1, epoch: 4. train_loss: 0.8469421926403206, train_acc: 83.60207515423444. val_loss: 0.9209174841333244, val_acc: 84.270293004346
kfold: 1, epoch: 5. train_loss: 0.8296085619318038, train_acc: 84.58356702187325. val_loss: 1.0381884839075568, val_acc: 84.21421561755223
kfold: 1, epoch: 6. train_loss: 0.8255002538240565, train_acc: 84.4643858665171. val_loss: 0.9199162993840153, val_acc: 86.33113696901724
kfold: 1, epoch: 7. train_loss: 0.8105724489444839, train_acc: 85.67722938867078. val_loss: 0.9234416891053373, val_acc: 86.72367867657367
kfold: 1, epoch: 8. train_loss: 0.8016543304626018, train_acc: 85.95064498037016. val_loss: 0.8698426490743053, val_acc: 86.55544651619235
kfold: 1, epoch: 9. train_loss: 0.7933315920214672, train_acc: 86.09085810431857. val_loss: 0.9627553278871167, val_acc: 85.61615028739661
kfold: 1, epoch: 10. train_loss: 0.7896827025421537, train_acc: 86.52551878855861. val_loss: 0.8496688226374184, val_acc: 86.75171736997056
kfold: 1, epoch: 11. train_loss: 0.7896756774845648, train_acc: 86.43438025799215. val_loss: 1.1489542242957325, val_acc: 84.85910556568064
kfold: 1, epoch: 12. train_loss: 0.7878786884979525, train_acc: 86.46242288278182. val_loss: 0.9898842008200085, val_acc: 84.98527968596663
kfold: 1, epoch: 13. train_loss: 0.7802066831226371, train_acc: 86.89708356702187. val_loss: 0.958517443709324, val_acc: 85.85447918127015
kfold: 1, epoch: 14. train_loss: 0.7730381936503473, train_acc: 87.24060572069546. val_loss: 0.8695904230383019, val_acc: 87.15827842422543
kfold: 1, epoch: 15. train_loss: 0.7705262340208284, train_acc: 87.44391475042065. val_loss: 0.9574083803179286, val_acc: 86.765736716669
kfold: 1, epoch: 16. train_loss: 0.7679714334525891, train_acc: 87.5701065619742. val_loss: 0.9919891364607184, val_acc: 86.20496284873126
kfold: 1, epoch: 17. train_loss: 0.770892263302894, train_acc: 87.52103196859225. val_loss: 1.0374552831283497, val_acc: 86.33113696901724
kfold: 1, epoch: 18. train_loss: 0.7578231412844249, train_acc: 87.92765002804262. val_loss: 1.4361531760534185, val_acc: 84.29833169774288
kfold: 1, epoch: 19. train_loss: 0.7567393042803212, train_acc: 88.01878855860909. val_loss: 0.9045880005110336, val_acc: 87.22837515771765
kfold: 1, epoch: 20. train_loss: 0.7558274767035122, train_acc: 88.16601233875491. val_loss: 0.9364005515045367, val_acc: 86.13486611523903
kfold: 1, epoch: 21. train_loss: 0.7550305722556084, train_acc: 88.03982052720134. val_loss: 0.9107737531506919, val_acc: 87.15827842422543
kfold: 1, epoch: 22. train_loss: 0.7513980882527517, train_acc: 88.22209758833426. val_loss: 1.0482575054391947, val_acc: 85.20958923314174
kfold: 1, epoch: 23. train_loss: 0.7473598969865233, train_acc: 88.69882220975883. val_loss: 0.9269738814260634, val_acc: 87.32651058460675
kfold: 1, epoch: 24. train_loss: 0.7486113026796668, train_acc: 88.34828939988783. val_loss: 0.983950479310669, val_acc: 85.95261460815927
kfold: 1, epoch: 25. train_loss: 0.7477202107497835, train_acc: 88.39736399326976. val_loss: 0.9916610498203762, val_acc: 86.13486611523903
kfold: 2, epoch: 1. train_loss: 1.086888437349332, train_acc: 72.30283911671924. val_loss: 0.9587364452679507, val_acc: 80.14582164890633
kfold: 2, epoch: 2. train_loss: 0.9141684537112387, train_acc: 80.55380301437084. val_loss: 0.8592885781123407, val_acc: 85.1233875490746
kfold: 2, epoch: 3. train_loss: 0.8711556577254659, train_acc: 82.56572029442692. val_loss: 0.8631291486912073, val_acc: 85.55804823331464
kfold: 2, epoch: 4. train_loss: 0.8529007146055327, train_acc: 83.18962495618646. val_loss: 0.9313911534532429, val_acc: 84.9831744251262
kfold: 2, epoch: 5. train_loss: 0.8381843096441496, train_acc: 84.19207851384508. val_loss: 0.9165240411202158, val_acc: 84.19798093101514
kfold: 2, epoch: 6. train_loss: 0.8274163846947544, train_acc: 84.65474938661059. val_loss: 0.8958121333407072, val_acc: 86.2450925406618
kfold: 2, epoch: 7. train_loss: 0.8197520374499714, train_acc: 84.84402383456012. val_loss: 0.8489670697484736, val_acc: 86.11890072910825
kfold: 2, epoch: 8. train_loss: 0.8107189943291404, train_acc: 85.068349106204. val_loss: 0.7969047410462722, val_acc: 87.38081884464385
kfold: 2, epoch: 9. train_loss: 0.8013984946437158, train_acc: 86.08482299334034. val_loss: 0.820513338519159, val_acc: 87.96971396522714
kfold: 2, epoch: 10. train_loss: 0.7994748144134405, train_acc: 85.75534525061339. val_loss: 0.8482105146280782, val_acc: 86.56758272574314
kfold: 2, epoch: 11. train_loss: 0.7935124131985772, train_acc: 86.29512793550649. val_loss: 0.93652316321166, val_acc: 84.9831744251262
kfold: 2, epoch: 12. train_loss: 0.7913618341378672, train_acc: 86.58254468980022. val_loss: 0.8555758271535594, val_acc: 87.59113853056647
kfold: 2, epoch: 13. train_loss: 0.7831127526849345, train_acc: 86.757798808272. val_loss: 0.9108780936718789, val_acc: 86.30117779024117
kfold: 2, epoch: 14. train_loss: 0.7790832199907296, train_acc: 87.1012968804767. val_loss: 0.8663193970099635, val_acc: 87.24060572069546
kfold: 2, epoch: 15. train_loss: 0.7787881387110664, train_acc: 87.33263231685945. val_loss: 0.8265995368219398, val_acc: 87.38081884464385
kfold: 2, epoch: 16. train_loss: 0.7738532798330028, train_acc: 87.18541885734315. val_loss: 0.8444365125256045, val_acc: 87.71733034212002
kfold: 2, epoch: 17. train_loss: 0.7700016116270356, train_acc: 87.54293725902559. val_loss: 0.8985754177601099, val_acc: 87.14245653393158
kfold: 2, epoch: 18. train_loss: 0.767836971218115, train_acc: 87.48685594111461. val_loss: 0.9816235785930931, val_acc: 87.22658440830061
kfold: 2, epoch: 19. train_loss: 0.7618673577242474, train_acc: 87.78128286014721. val_loss: 0.8954949683174951, val_acc: 86.97420078519349
kfold: 2, epoch: 20. train_loss: 0.7627530407390838, train_acc: 87.8583946722748. val_loss: 0.9587337172272095, val_acc: 86.56758272574314
kfold: 2, epoch: 21. train_loss: 0.7633227513011268, train_acc: 87.54994742376446. val_loss: 0.8293302527853965, val_acc: 88.27818283791363
kfold: 2, epoch: 22. train_loss: 0.7550378550190285, train_acc: 88.14581142656853. val_loss: 0.8723698158947457, val_acc: 87.87156477846327
kfold: 2, epoch: 23. train_loss: 0.7542703681234122, train_acc: 88.19488257974062. val_loss: 1.0074993828442147, val_acc: 86.2450925406618
kfold: 2, epoch: 24. train_loss: 0.7467880458178668, train_acc: 88.53838065194532. val_loss: 0.9342621168893706, val_acc: 87.17049915872126
kfold: 2, epoch: 25. train_loss: 0.7499959952079546, train_acc: 88.53838065194532. val_loss: 1.063686335280713, val_acc: 86.13292204150308
kfold: 3, epoch: 1. train_loss: 1.078337675923805, train_acc: 72.70942867157378. val_loss: 0.9213816992232322, val_acc: 82.08076275939428
kfold: 3, epoch: 2. train_loss: 0.9003760502396234, train_acc: 81.1777076761304. val_loss: 0.9281291508199924, val_acc: 83.39876612450925
kfold: 3, epoch: 3. train_loss: 0.8639528176718668, train_acc: 82.73396424815984. val_loss: 0.8943628671960889, val_acc: 85.20751542344364
kfold: 3, epoch: 4. train_loss: 0.849780801584957, train_acc: 83.4770417104802. val_loss: 0.9418105358530549, val_acc: 83.56702187324734
kfold: 3, epoch: 5. train_loss: 0.830580298694708, train_acc: 84.17104801962846. val_loss: 0.8897512886240719, val_acc: 85.62815479528884
kfold: 3, epoch: 6. train_loss: 0.817620327266122, train_acc: 85.14546091833158. val_loss: 0.8705632920119446, val_acc: 85.93662366797533
kfold: 3, epoch: 7. train_loss: 0.8069939942325163, train_acc: 85.71328426218017. val_loss: 0.9264021347863448, val_acc: 85.51598429613011