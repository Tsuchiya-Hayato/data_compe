filename: 003_albumentations_smoothing.ipynb, model: efficientnet-b5, lr: 1e-05, weights: tensor([1.5000, 1.0000, 1.0000, 1.0000, 1.0000]). batchsize: 2, kfold: 3, epoch: 25, weght_decay: 5e-05, smoothing: 0.1

kfold: 1, epoch: 1. train_loss: 1.2893474619925456, train_acc: 62.50701065619742. val_loss: 1.4024905773448382, val_acc: 61.19444833870742
kfold: 1, epoch: 2. train_loss: 1.1730298854899555, train_acc: 67.75098149186763. val_loss: 1.2232092032921826, val_acc: 68.58264404878733
kfold: 1, epoch: 3. train_loss: 1.1197999608078768, train_acc: 71.27734155916994. val_loss: 1.1470538364728025, val_acc: 69.80232721155194
kfold: 1, epoch: 4. train_loss: 1.062065052872485, train_acc: 73.89231632080762. val_loss: 1.0573821225218456, val_acc: 74.75115659610262
kfold: 1, epoch: 5. train_loss: 1.0031101822568937, train_acc: 76.80173864273696. val_loss: 1.0165823912433392, val_acc: 76.6297490536941
kfold: 1, epoch: 6. train_loss: 0.9612527219886802, train_acc: 78.61749859786876. val_loss: 0.9913486765485068, val_acc: 78.59245759147623
kfold: 1, epoch: 7. train_loss: 0.929222124651183, train_acc: 79.54991587212564. val_loss: 0.9826814068911087, val_acc: 80.06448899481285
kfold: 1, epoch: 8. train_loss: 0.9063551159334263, train_acc: 81.04318564217611. val_loss: 1.019688116842403, val_acc: 79.54577316697042
kfold: 1, epoch: 9. train_loss: 0.8855062577825413, train_acc: 81.97560291643298. val_loss: 0.9659405300053756, val_acc: 80.59722416935371
kfold: 1, epoch: 10. train_loss: 0.8775748295925737, train_acc: 82.62759394279304. val_loss: 1.0042693142066124, val_acc: 80.58320482265526
kfold: 1, epoch: 11. train_loss: 0.8737665916167814, train_acc: 82.56449803701626. val_loss: 0.9372058806714806, val_acc: 82.8963970278985
kfold: 1, epoch: 12. train_loss: 0.8603309016764599, train_acc: 83.1674144699944. val_loss: 0.9585760377654516, val_acc: 83.14874526847049
kfold: 1, epoch: 13. train_loss: 0.8512525402022023, train_acc: 83.67218171620864. val_loss: 0.9218256805736477, val_acc: 83.27491938875649
kfold: 1, epoch: 14. train_loss: 0.8440368982158346, train_acc: 84.31015143017386. val_loss: 0.9231126651497893, val_acc: 83.49922893593158
kfold: 1, epoch: 15. train_loss: 0.8354085754860137, train_acc: 84.45036455412226. val_loss: 0.9237736988575611, val_acc: 83.89177064348802
kfold: 1, epoch: 16. train_loss: 0.8312840048209108, train_acc: 84.7378014582165. val_loss: 0.924208746158278, val_acc: 83.2468806953596
kfold: 1, epoch: 17. train_loss: 0.8313831479711207, train_acc: 84.6396522714526. val_loss: 0.9139877826785854, val_acc: 83.8216739099958
kfold: 1, epoch: 18. train_loss: 0.822323690629781, train_acc: 85.10936623667975. val_loss: 0.9250366902331327, val_acc: 83.07864853497827
kfold: 1, epoch: 19. train_loss: 0.8186168739936757, train_acc: 85.31267526640494. val_loss: 0.8766262479479453, val_acc: 85.1675311930464
kfold: 1, epoch: 20. train_loss: 0.8159972255191841, train_acc: 85.36174985978688. val_loss: 0.88389727893516, val_acc: 84.3684284312351
kfold: 1, epoch: 21. train_loss: 0.8107891723605534, train_acc: 85.51598429613011. val_loss: 0.8549933536299343, val_acc: 85.40586008691994
kfold: 1, epoch: 22. train_loss: 0.806255901613666, train_acc: 85.67021873247336. val_loss: 0.8797393076587599, val_acc: 84.17215757745689
kfold: 1, epoch: 23. train_loss: 0.8032078857142405, train_acc: 85.94363432417275. val_loss: 0.8558819122865216, val_acc: 85.72830506098416
kfold: 1, epoch: 24. train_loss: 0.8018919188890709, train_acc: 86.0347728547392. val_loss: 0.8772817620684327, val_acc: 84.34038973783822
kfold: 1, epoch: 25. train_loss: 0.7978468498552508, train_acc: 86.13292204150308. val_loss: 0.8463441935078851, val_acc: 85.58811159399971
kfold: 2, epoch: 1. train_loss: 1.300348399732652, train_acc: 62.208201892744476. val_loss: 1.343300053579247, val_acc: 62.6752664049355
kfold: 2, epoch: 2. train_loss: 1.1699912748446426, train_acc: 68.06869961444094. val_loss: 1.2583841962120337, val_acc: 64.32978126752664
kfold: 2, epoch: 3. train_loss: 1.1083869970682108, train_acc: 71.55275148966001. val_loss: 1.1175354637928512, val_acc: 71.28435221536736
kfold: 2, epoch: 4. train_loss: 1.0539709726941546, train_acc: 73.99929898352612. val_loss: 1.0243345799736274, val_acc: 75.77117218171621
kfold: 2, epoch: 5. train_loss: 1.0075563441632456, train_acc: 75.82194181563267. val_loss: 1.0301711772909607, val_acc: 75.350532809871
kfold: 2, epoch: 6. train_loss: 0.9610748026876719, train_acc: 77.97406239046617. val_loss: 0.9925631496321934, val_acc: 78.81379697139653
kfold: 2, epoch: 7. train_loss: 0.9304269972212532, train_acc: 79.63547143357869. val_loss: 0.9966260336017716, val_acc: 79.23443634324173
kfold: 2, epoch: 8. train_loss: 0.9117603826435923, train_acc: 80.92534174553101. val_loss: 0.9964747056156079, val_acc: 79.83735277621986
kfold: 2, epoch: 9. train_loss: 0.8931703303700224, train_acc: 81.48615492464073. val_loss: 1.0613763739370057, val_acc: 78.49130678631519
kfold: 2, epoch: 10. train_loss: 0.880698869668901, train_acc: 82.36242551699965. val_loss: 1.020487228622613, val_acc: 80.58048233314638
kfold: 2, epoch: 11. train_loss: 0.8784580619943839, train_acc: 82.53066947073256. val_loss: 1.03327533178161, val_acc: 79.78126752664049
kfold: 2, epoch: 12. train_loss: 0.8661867014225011, train_acc: 82.72695408342096. val_loss: 0.9932972867369853, val_acc: 82.3331463825014
kfold: 2, epoch: 13. train_loss: 0.8591341206423451, train_acc: 83.26673676831406. val_loss: 1.0021898824548159, val_acc: 82.44531688166012
kfold: 2, epoch: 14. train_loss: 0.8514769386814, train_acc: 83.81352961794602. val_loss: 0.9507865859127687, val_acc: 84.07178911946158
kfold: 2, epoch: 15. train_loss: 0.847199268675646, train_acc: 83.61724500525763. val_loss: 0.9381395108199695, val_acc: 83.56702187324734
kfold: 2, epoch: 16. train_loss: 0.8419458797264446, train_acc: 84.35331230283911. val_loss: 0.9177254531501186, val_acc: 85.08132361189007
kfold: 2, epoch: 17. train_loss: 0.8314831824251199, train_acc: 84.74588152821592. val_loss: 0.9759252110994471, val_acc: 83.31463825014022