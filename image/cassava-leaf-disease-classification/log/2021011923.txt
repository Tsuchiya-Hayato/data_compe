model: efficientnet-b3, lr: 1e-05, weights: tensor([1.5000, 1.0000, 1.0000, 1.0000, 1.0000]). batchsize: 8, kfold: 3, epoch: 20, RandomHorizontalFlip:(True,),CenterCrop: True 

kfold: 1, epoch: 1. train_loss: 1.0325239150629777, train_acc: 64.32277061132922. val_loss: 0.815341555718082, val_acc: 74.5548857423244
kfold: 1, epoch: 2. train_loss: 0.7598017602323216, train_acc: 73.6399326977005. val_loss: 0.6529643045228827, val_acc: 78.35412869760269
kfold: 1, epoch: 3. train_loss: 0.6431610981484043, train_acc: 77.50981491867638. val_loss: 0.570967040601032, val_acc: 80.37291462217861
kfold: 1, epoch: 4. train_loss: 0.594108411266071, train_acc: 79.255468311834. val_loss: 0.536404246456019, val_acc: 81.71877190522922
kfold: 1, epoch: 5. train_loss: 0.5541551532166364, train_acc: 80.81183398766125. val_loss: 0.517049355168238, val_acc: 82.58797140053274
kfold: 1, epoch: 6. train_loss: 0.5194236996154804, train_acc: 82.15086932136848. val_loss: 0.5009268903426175, val_acc: 83.07864853497827
kfold: 1, epoch: 7. train_loss: 0.4884962215183895, train_acc: 83.39175546831183. val_loss: 0.49493268649031164, val_acc: 83.45717089583626
kfold: 1, epoch: 8. train_loss: 0.47608733409937354, train_acc: 83.58104318564217. val_loss: 0.4854722747614716, val_acc: 83.73755782980513
kfold: 1, epoch: 9. train_loss: 0.4551500647411845, train_acc: 84.05075715086932. val_loss: 0.49110684544161265, val_acc: 83.73755782980513
kfold: 1, epoch: 10. train_loss: 0.43415068534991075, train_acc: 85.06730229949524. val_loss: 0.48738025125552537, val_acc: 84.35440908453666
kfold: 1, epoch: 11. train_loss: 0.41091747496713765, train_acc: 85.80342120022434. val_loss: 0.48492717013105485, val_acc: 84.67685405860087
kfold: 1, epoch: 12. train_loss: 0.3953074228239992, train_acc: 86.56758272574314. val_loss: 0.49218965488728217, val_acc: 83.98990607037712
kfold: 1, epoch: 13. train_loss: 0.36788235453286, train_acc: 87.23359506449803. val_loss: 0.4919310851532114, val_acc: 84.41048647133043
kfold: 1, epoch: 14. train_loss: 0.3545781730852323, train_acc: 87.71733034212002. val_loss: 0.5128113945071416, val_acc: 83.5973643628207
kfold: 1, epoch: 15. train_loss: 0.33593683744625585, train_acc: 88.52355580482333. val_loss: 0.5151200699371653, val_acc: 83.76559652320202
kfold: 1, epoch: 16. train_loss: 0.32277337202370854, train_acc: 88.8250140213124. val_loss: 0.5143309830690028, val_acc: 83.38707416234404
kfold: 1, epoch: 17. train_loss: 0.30840709519196996, train_acc: 89.3157599551318. val_loss: 0.5202914682987476, val_acc: 83.42913220243936
kfold: 1, epoch: 18. train_loss: 0.29972953559487087, train_acc: 89.89764441951766. val_loss: 0.5436845837879275, val_acc: 83.00855180148605
kfold: 1, epoch: 19. train_loss: 0.27056399997677893, train_acc: 90.90717891194616. val_loss: 0.5337724576901536, val_acc: 83.70951913640825
kfold: 1, epoch: 20. train_loss: 0.26708121742453556, train_acc: 90.90717891194616. val_loss: 0.5526726707104059, val_acc: 83.61138370951913
kfold: 2, epoch: 1. train_loss: 1.0255402484699883, train_acc: 65.5800911321416. val_loss: 0.8805171483835297, val_acc: 73.26135726303983
kfold: 2, epoch: 2. train_loss: 0.7723929382058804, train_acc: 73.23519102698913. val_loss: 0.6796362438351918, val_acc: 78.29500841278744
kfold: 2, epoch: 3. train_loss: 0.6674629503510258, train_acc: 76.81037504381354. val_loss: 0.6005790460470546, val_acc: 79.55692652832305
kfold: 2, epoch: 4. train_loss: 0.6006775602088581, train_acc: 78.99754644234139. val_loss: 0.5435686233545332, val_acc: 81.28154795288839
kfold: 2, epoch: 5. train_loss: 0.5587461251690315, train_acc: 80.4416403785489. val_loss: 0.5265320825219288, val_acc: 81.42176107683679
kfold: 2, epoch: 6. train_loss: 0.5361357215842056, train_acc: 81.35997195934104. val_loss: 0.5094332703631455, val_acc: 82.0527201346046
kfold: 2, epoch: 7. train_loss: 0.5189436675266534, train_acc: 82.15913073957238. val_loss: 0.5020619614024734, val_acc: 82.75378575434661
kfold: 2, epoch: 8. train_loss: 0.4838153722554011, train_acc: 83.29477742726954. val_loss: 0.4911014424123271, val_acc: 83.14638250140213
kfold: 2, epoch: 9. train_loss: 0.4649990629676245, train_acc: 84.15702769015071. val_loss: 0.48612098197657366, val_acc: 82.86595625350533
kfold: 2, epoch: 10. train_loss: 0.44404232651496417, train_acc: 84.74588152821592. val_loss: 0.4807080543827633, val_acc: 83.51093662366797
kfold: 2, epoch: 11. train_loss: 0.4240399321818916, train_acc: 85.5800911321416. val_loss: 0.47935733112095197, val_acc: 83.37072349971957
kfold: 2, epoch: 12. train_loss: 0.4105328419976638, train_acc: 85.95162986330179. val_loss: 0.48179833304331193, val_acc: 83.86146943353899
kfold: 2, epoch: 13. train_loss: 0.3786160231770471, train_acc: 86.76480897301087. val_loss: 0.49009796802628985, val_acc: 83.70723499719574
kfold: 2, epoch: 14. train_loss: 0.36274612236939235, train_acc: 87.6621100595864. val_loss: 0.4952012589018617, val_acc: 83.1043185642176
kfold: 2, epoch: 15. train_loss: 0.3506630361570889, train_acc: 87.85138450753593. val_loss: 0.5013459632144468, val_acc: 83.30061693774537
kfold: 2, epoch: 16. train_loss: 0.338524022316059, train_acc: 88.69961444093936. val_loss: 0.5062645506001529, val_acc: 83.00616937745373
kfold: 2, epoch: 17. train_loss: 0.3152037760255349, train_acc: 89.26743778478794. val_loss: 0.5101206425865569, val_acc: 83.31463825014022
kfold: 2, epoch: 18. train_loss: 0.3028774417027405, train_acc: 89.75814931650893. val_loss: 0.517545169797797, val_acc: 83.32865956253505
kfold: 2, epoch: 19. train_loss: 0.27722870499405156, train_acc: 90.49421661409043. val_loss: 0.5385785825668538, val_acc: 83.2024677509815
kfold: 2, epoch: 20. train_loss: 0.2694672430283912, train_acc: 90.68349106203996. val_loss: 0.541678888234101, val_acc: 83.49691531127314
kfold: 3, epoch: 1. train_loss: 1.0371072760077336, train_acc: 64.36032246757799. val_loss: 0.824424833610587, val_acc: 74.04655075715087
kfold: 3, epoch: 2. train_loss: 0.7554873549817923, train_acc: 74.10445145460919. val_loss: 0.6520456158693864, val_acc: 77.46775098149186
kfold: 3, epoch: 3. train_loss: 0.6505898968170443, train_acc: 77.54644234139502. val_loss: 0.5775129350086031, val_acc: 79.87941671340437
kfold: 3, epoch: 4. train_loss: 0.5877406593295224, train_acc: 79.77567472835612. val_loss: 0.5391714539558464, val_acc: 81.21144139091419
kfold: 3, epoch: 5. train_loss: 0.5496717025484105, train_acc: 81.00245355765861. val_loss: 0.5212802406141509, val_acc: 81.73022994952328
kfold: 3, epoch: 6. train_loss: 0.5354483131102115, train_acc: 81.64738871363477. val_loss: 0.496776262518528, val_acc: 82.51542344363432
kfold: 3, epoch: 7. train_loss: 0.500300657578364, train_acc: 83.00035050823695. val_loss: 0.4914848214357824, val_acc: 83.21648906337633
kfold: 3, epoch: 8. train_loss: 0.47398009303865346, train_acc: 83.84858044164038. val_loss: 0.4730241650876259, val_acc: 83.86146943353899
kfold: 3, epoch: 9. train_loss: 0.4486853060659205, train_acc: 84.64773922187172. val_loss: 0.4755195658883673, val_acc: 83.67919237240606
kfold: 3, epoch: 10. train_loss: 0.4378302827197641, train_acc: 85.12443042411496. val_loss: 0.4819564689514582, val_acc: 83.23051037577117
kfold: 3, epoch: 11. train_loss: 0.4167171818717913, train_acc: 85.57308096740273. val_loss: 0.47420143984120355, val_acc: 84.02972518227706
kfold: 3, epoch: 12. train_loss: 0.3991344558031219, train_acc: 86.4143007360673. val_loss: 0.479563468169304, val_acc: 83.60908581043185
kfold: 3, epoch: 13. train_loss: 0.38260940404038773, train_acc: 87.08026638626008. val_loss: 0.469047090635351, val_acc: 84.5344924284913
kfold: 3, epoch: 14. train_loss: 0.36105773845090294, train_acc: 87.84437434279705. val_loss: 0.49529646678552064, val_acc: 83.65114974761637
kfold: 3, epoch: 15. train_loss: 0.34116409624709576, train_acc: 88.2719943918682. val_loss: 0.4894989881061034, val_acc: 84.00168255748738
kfold: 3, epoch: 16. train_loss: 0.3306558996725059, train_acc: 88.74868559411146. val_loss: 0.491741510666698, val_acc: 83.88951205832866